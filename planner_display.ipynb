{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLANNING GRAPH PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "from scipy import stats\n",
    "from planning_graph.planning_graph import PlanningGraph, NoOpAction\n",
    "from pddlpy.pddl import Operator\n",
    "from typing import Set, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish graph\n",
      "LayeredPlan. Levels=11\n",
      "0\n",
      "1\n",
      "unstack\n",
      "2\n",
      "put-down\n",
      "3\n",
      "unstack\n",
      "4\n",
      "put-down\n",
      "5\n",
      "pick-up\n",
      "6\n",
      "stack\n",
      "7\n",
      "pick-up\n",
      "8\n",
      "stack\n",
      "9\n",
      "pick-up\n",
      "10\n",
      "stack\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "from planning_graph.planning_graph import PlanningGraph\n",
    "from planning_graph.planning_graph_planner import GraphPlanner\n",
    "\n",
    "\n",
    "planning_graph = PlanningGraph(\"domain/blocks/domain.pddl\", \n",
    "                               'domain/blocks/blocks/blocks4/task51.pddl')\n",
    "graph = planning_graph.create(max_num_of_levels=100)\n",
    "print(\"finish graph\")\n",
    "goal = planning_graph.goal\n",
    "graph_planner = GraphPlanner()\n",
    "layered_plan = graph_planner.plan(graph, goal, planning_graph)\n",
    "print(layered_plan)\n",
    "t = 0\n",
    "for i in range(len(layered_plan._layered_plan)):\n",
    "    print(i)\n",
    "    for a in (layered_plan[i]._plan):\n",
    "        if not isinstance(a, NoOpAction):\n",
    "            print(a.operator_name)\n",
    "            t+=1\n",
    "            \n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_vec(planning_graph, state, max_level, visual = False, title = None):\n",
    "    \"\"\"\n",
    "    Generate the feature vector followed by the paper from the input (problem, state) pair as described by the paper\n",
    "    \n",
    "    Inputs\n",
    "    ----------\n",
    "    planning_graph: the problem pi\n",
    "    state: the current state s\n",
    "    max_level: the maximum layer allowed for ff algorithm to do forward expanding\n",
    "    visual/title: whether need to generate a photo representing the graph, default False/None, \n",
    "        if need to generate a photo with name as title, visual must be true and title must not be none\n",
    "    \n",
    "    Outputs\n",
    "    ----------\n",
    "    feature_vec: a vector of length n + 2*n**2 + 3 representing the feature generated from the given (problem, state) pair\n",
    "                 the first n values are single action feature\n",
    "                 the second 2*n**2 are pairwise action feature\n",
    "                 the last 3 are original heuristic value, the number of layers in pi and the number of unsatisfied goals\n",
    "    \"\"\"\n",
    "    # draw graph first\n",
    "    graph = planning_graph.create_with_state(state, max_num_of_levels=max_level, visualize = visual)\n",
    "    if title is not None and visual:\n",
    "        graph.visualize_png(title)\n",
    "    \n",
    "    # get action schema and output list\n",
    "    act_schema = np.array(list(planning_graph._planning_problem._domain_problem.operators())) # store names of total action schema\n",
    "    n = len(act_schema)  # length of action schema\n",
    "    feature_vec = np.zeros(n+2*n**2+3) # return feature vec, first n is linear, second 2*n**2 is pairwise, last 3 is additional feature\n",
    "    act_layers = list(graph.act.values()) # list of layers generated, ith value is the list of actions connencting i-1 th states to ith states layer\n",
    "\n",
    "    # extract linear feature\n",
    "    #-------------------------------------\n",
    "    # ith value indicate the num of occurance for ith action of act_schema in the entire graph \n",
    "    counter = np.zeros(n)\n",
    "    for act_layer in act_layers:\n",
    "        if act_layer is not None:\n",
    "            for a in act_layer:\n",
    "                 if not isinstance(a, NoOpAction):\n",
    "                        counter[act_schema == a.operator_name] += 1\n",
    "    feature_vec[0:n] = counter\n",
    "    \n",
    "    # extract pair-wise feature\n",
    "    #-------------------------------------\n",
    "    # each pair a1, a2 is stored in n + [2*(n*a1+a2), 2*(n*a1+a2)+1]\n",
    "    # e.g. when a1 is 1, a2 is 3, n is 5, store in 5 + [2*(8),  2*(8)+1]\n",
    "    def to_index(n, index_a1, index_a2, adder):\n",
    "        \"\"\"\n",
    "        return corresponding index in the position of the feature vector\n",
    "        adder is either 0 or 1\n",
    "        index_a1, index_a2 refer to move index in act_schema\n",
    "        \"\"\"\n",
    "        return n+2*(n*index_a1+index_a2)+adder\n",
    "    \n",
    "    \n",
    "    def append_to_dict(a, pre, eff_pos):\n",
    "        \"\"\"\n",
    "        add action a into the dicitonary pre and eff_pos\n",
    "        \"\"\"\n",
    "        for p in a.precondition_pos:\n",
    "            current = pre.get(p)\n",
    "            if current is None:\n",
    "                current = [a]\n",
    "            else:\n",
    "                current.append(a)\n",
    "            pre[p] = current\n",
    "            \n",
    "        for p in a.effect_pos:\n",
    "            current = eff_pos.get(p)\n",
    "            if current is None:\n",
    "                current = [a]\n",
    "            else:\n",
    "                current.append(a)\n",
    "            eff_pos[p] = current\n",
    "            \n",
    "#         for p in a.effect_neg:\n",
    "#             current = effect_neg.get(p)\n",
    "#             if current is None:\n",
    "#                 current = [name]\n",
    "#             else:\n",
    "#                 current.append(name)\n",
    "#             effect_neg[p] = current\n",
    "#         return pre, eff_pos, eff_neg\n",
    "            return pre, eff_pos\n",
    "\n",
    "\n",
    "    # define dictionary variables for comparison purpose\n",
    "    pre = {}\n",
    "    eff_pos = {}\n",
    "#     eff_neg = {}\n",
    "    \n",
    "    # add pre and eff into the empty dictionary for the first layer\n",
    "    for a in act_layers[1]:\n",
    "        if not isinstance(a, NoOpAction):\n",
    "            pre, eff_pos = append_to_dict(a, pre, eff_pos)\n",
    "   \n",
    "    # loop through second to last action layers\n",
    "    for i in range(2,len(act_layers)): \n",
    "        act_layer = act_layers[i]\n",
    "        \n",
    "        # update fecture vec for the entire layer\n",
    "        for a2 in act_layer:\n",
    "            if not isinstance(a2, NoOpAction):\n",
    "                # count for num of occurances, use set to avoid multiple countsc\n",
    "                s1 = set() # feature 1 where eff a1 and pre a2 has intersections\n",
    "                s2 = set() # feature 2 where pre a1 and eff a2 has intersections          \n",
    "                for p in a2.precondition_pos:\n",
    "                    current = eff_pos.get(p)\n",
    "                    if current is not None:\n",
    "                        for a1 in current:\n",
    "                            s1.add(a1) \n",
    "\n",
    "                for p in a2.effect_pos:\n",
    "                    current = pre.get(p)\n",
    "                    if current is not None:\n",
    "                        for a1 in current:\n",
    "                            s2.add(a1)\n",
    "                            \n",
    "                # add index to feature_vec based on set generated:\n",
    "                index_a2 = int(np.where(a2.operator_name == act_schema)[0])\n",
    "                for a1 in s1:\n",
    "                    # update feature 1 for pair (a1, a2)\n",
    "                    index_a1 = int(np.where(a1.operator_name == act_schema)[0])\n",
    "                    feature_vec[to_index(n, index_a1, index_a2,0)]+=1\n",
    "      \n",
    "                for a1 in s2:\n",
    "                    # update feature 2 for pair (a1, a2)\n",
    "                    index_a1 = int(np.where(a1.operator_name == act_schema)[0])\n",
    "                    feature_vec[to_index(n, index_a1, index_a2,1)]+=1\n",
    "\n",
    "        # update pre and eff_pos for the entire layer\n",
    "        for a2 in act_layer:\n",
    "            if not isinstance(a2, NoOpAction):\n",
    "                pre, eff_pos = append_to_dict(a2, pre, eff_pos)\n",
    "           \n",
    "    # extract final features\n",
    "    #-------------------------------------\n",
    "    # add heuristic value, number of layers and number of unsatisfied goals\n",
    "    goal = planning_graph.goal\n",
    "    graph_planner = GraphPlanner()\n",
    "    layered_plan = graph_planner.plan(graph, goal, planning_graph)\n",
    "    total_len = len(feature_vec)\n",
    "    # number of layers:\n",
    "    feature_vec[total_len - 3] = len(act_layers)\n",
    "    # heuristic value hFF: (number of total actions in the plan)\n",
    "    h_v = 0\n",
    "    for i in range(len(layered_plan._layered_plan)):\n",
    "        for a in (layered_plan[i]._plan):\n",
    "            if not isinstance(a, NoOpAction):\n",
    "                h_v += 1\n",
    "    feature_vec[total_len - 2] = h_v\n",
    "    \n",
    "    ### ISSUE: UNSATISFIED GOAL FEATURE\n",
    "    #-------------------------------------\n",
    "#     # unsatisfied goal (2 ** (last layer total pos num - goal state pos num))\n",
    "#     last = graph.prop[len(graph.prop)-1]\n",
    "#     feature_vec[total_len - 1] = 2 ** (len(last) - len(goal))\n",
    "    #-------------------------------------\n",
    "    \n",
    "\n",
    "    return feature_vec\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_operator(action : str, op_list: List[Operator]):\n",
    "    \"\"\"\n",
    "    find an operator from the planning graph's ground operator lists\n",
    "    \n",
    "    return: the action operator if found\n",
    "    \"\"\"\n",
    "    name_list = action.replace('(', '').replace(')', '').split(' ')\n",
    "    for op in op_list:\n",
    "        if op.operator_name == name_list[0]:\n",
    "            if list(op.variable_list.values()) == name_list[1:]: return op\n",
    "    return None\n",
    "\n",
    "def apply_operation(action: Operator, state : Set[Tuple]):\n",
    "    \"\"\"\n",
    "    apply an action onto the input state\n",
    "    \n",
    "    return: the new state\n",
    "    \"\"\"\n",
    "    new_state = state.copy()\n",
    "    for eff in action.effect_pos:\n",
    "        new_state.add(eff)\n",
    "    for eff in action.effect_neg:\n",
    "        new_state.remove(eff)\n",
    "    return new_state\n",
    "\n",
    "def read_plan(plan_file_path: str):\n",
    "    \"\"\"\n",
    "    read all the lines from a plan file directory, remove the last line containing cost\n",
    "    \n",
    "    return: a list containing the ground truth plan with length equal to total cost\n",
    "    \"\"\"\n",
    "    with open(plan_file_path, \"r\") as f:\n",
    "\n",
    "        # Read the lines of the file into a list of strings\n",
    "        lines = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    return lines[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def generate_training_data(domain_file_path, task_file_path, plan_file_path, problem_num : int, visual = False):\n",
    "    \"\"\"\n",
    "    generate the feature vector matrix X together with a cost vector y from the given input\n",
    "    \n",
    "    Inputs\n",
    "    ----------\n",
    "    domain_file_path: the input domain file\n",
    "    task_file_path: the input problem file\n",
    "    plan_file_path: the input log file that store the optimal plan\n",
    "    problem_num: the problem index for this domain\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    None, None if no plan can be found (plan has cost 0)\n",
    "    X : array, shape (plan_length-1, n_features)\n",
    "        The input feature vec of states from initial states all the way towards the second-last state (one state before goal state)\n",
    "    Y : array, shape (plan_length-1, 2)\n",
    "        The input cost vector. If it's a 2D array\n",
    "        The first column is the true cost pi optimal (assume unit cost)\n",
    "        The second column is the probelm_num representing the index of this problem\n",
    "    \n",
    "    \"\"\"\n",
    "    # generate plan and get max level\n",
    "    plan_actions = read_plan(plan_file_path)\n",
    "    if len(plan_actions) == 0:\n",
    "        return None, None\n",
    "    max_level = max(10, len(plan_actions)+1)\n",
    "    max_level = len(plan_actions)+2\n",
    "    \n",
    "    # generate graph\n",
    "    planning_graph = PlanningGraph(domain_file_path, task_file_path, visualize = visual)\n",
    "    graph = planning_graph.create(max_num_of_levels = max_level)\n",
    "    ground_operators = planning_graph._planning_problem._get_ground_operators()\n",
    "    \n",
    "    # define output matrixes\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # loop from the final plan\n",
    "    current_state = planning_graph._planning_problem.initial_state\n",
    "    current_cost = len(plan_actions)\n",
    "    for i in range(0,len(plan_actions)-1):\n",
    "        X.append(generate_feature_vec(planning_graph, current_state, max_level))\n",
    "#         X.append(generate_feature_vec(planning_graph, current_state, max_level, visual = True, title = f\"test_image{i}th layer.png\"))\n",
    "        y.append(current_cost)\n",
    "        current_action = find_operator(plan_actions[i], ground_operators)\n",
    "        current_state = apply_operation(current_action, current_state)\n",
    "        current_cost -=1\n",
    "        \n",
    "    y = np.c_[y, problem_num * np.ones(len(y))]\n",
    "    return np.asarray(X), np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_training_data('domain/blocks/domain.pddl', 'domain/blocks/blocks/blocks3/task42.pddl', 'domain/blocks/plans/blocks3-task42_1800.out', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8536\\3311434441.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)\n",
    "print(type(X), type(y))\n",
    "print(X)\n",
    "print(y)\n",
    "print(type(y[0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATING DATA PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def generate_problem_matrix(domain_file_path, problem_folder_path, log_folder_path, output_path, title):\n",
    "    \n",
    "    # get the training problem names\n",
    "    problem_name_list = [f.split('.')[0] for f in os.listdir(problem_folder_path)]\n",
    "#     problem_name_list = [\"task\"+ str(i) for i in range(10,50)]\n",
    "    X = None\n",
    "    Y = None\n",
    "    problem_index = 0\n",
    "    \n",
    "#     # filter the too complex problems\n",
    "#     p_l = problem_name_list.copy()\n",
    "#     problem_name_list = []\n",
    "#     for p in p_l:\n",
    "#         if int(p[6])>=3 and int(p[6]) <= 6:\n",
    "#             problem_name_list.append(p)\n",
    "#     print(problem_name_list)\n",
    "    \n",
    "    # generate the final vectors\n",
    "    for prob in problem_name_list:\n",
    "        problem_path = problem_folder_path + \"/\" + prob + \".pddl\"\n",
    "        plan_path = log_folder_path + \"/blocks3-\" + prob + \"_1800.out\"\n",
    "        temp_X, temp_Y = generate_training_data(domain_file_path, problem_path, plan_path, problem_index)\n",
    "        if X is None:\n",
    "            X = temp_X\n",
    "            Y = temp_Y\n",
    "        elif temp_X is not None:\n",
    "            X = np.vstack((X, temp_X))\n",
    "            Y = np.vstack((Y, temp_Y))\n",
    "            print(problem_path,X.shape, Y.shape)\n",
    "        problem_index += 1\n",
    "        \n",
    "        \n",
    "    # save the final training vectors\n",
    "    np.savez(output_path+\"/\"+title+\".npz\", feature = X, label = Y)\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain/blocks/blocks/blocks3/task11.pddl (8, 39) (8, 2)\n",
      "domain/blocks/blocks/blocks3/task12.pddl (15, 39) (15, 2)\n",
      "domain/blocks/blocks/blocks3/task13.pddl (20, 39) (20, 2)\n",
      "domain/blocks/blocks/blocks3/task14.pddl (25, 39) (25, 2)\n",
      "domain/blocks/blocks/blocks3/task15.pddl (30, 39) (30, 2)\n",
      "domain/blocks/blocks/blocks3/task16.pddl (31, 39) (31, 2)\n",
      "domain/blocks/blocks/blocks3/task17.pddl (34, 39) (34, 2)\n",
      "domain/blocks/blocks/blocks3/task18.pddl (41, 39) (41, 2)\n",
      "domain/blocks/blocks/blocks3/task19.pddl (46, 39) (46, 2)\n",
      "domain/blocks/blocks/blocks3/task20.pddl (49, 39) (49, 2)\n",
      "domain/blocks/blocks/blocks3/task21.pddl (52, 39) (52, 2)\n",
      "domain/blocks/blocks/blocks3/task22.pddl (59, 39) (59, 2)\n",
      "domain/blocks/blocks/blocks3/task23.pddl (64, 39) (64, 2)\n",
      "domain/blocks/blocks/blocks3/task24.pddl (71, 39) (71, 2)\n",
      "domain/blocks/blocks/blocks3/task25.pddl (78, 39) (78, 2)\n",
      "domain/blocks/blocks/blocks3/task26.pddl (81, 39) (81, 2)\n",
      "domain/blocks/blocks/blocks3/task27.pddl (84, 39) (84, 2)\n",
      "domain/blocks/blocks/blocks3/task28.pddl (89, 39) (89, 2)\n",
      "domain/blocks/blocks/blocks3/task29.pddl (92, 39) (92, 2)\n",
      "domain/blocks/blocks/blocks3/task30.pddl (95, 39) (95, 2)\n",
      "domain/blocks/blocks/blocks3/task31.pddl (96, 39) (96, 2)\n",
      "domain/blocks/blocks/blocks3/task32.pddl (99, 39) (99, 2)\n",
      "domain/blocks/blocks/blocks3/task33.pddl (100, 39) (100, 2)\n",
      "domain/blocks/blocks/blocks3/task34.pddl (105, 39) (105, 2)\n",
      "domain/blocks/blocks/blocks3/task35.pddl (106, 39) (106, 2)\n",
      "domain/blocks/blocks/blocks3/task36.pddl (109, 39) (109, 2)\n",
      "domain/blocks/blocks/blocks3/task37.pddl (114, 39) (114, 2)\n",
      "domain/blocks/blocks/blocks3/task38.pddl (117, 39) (117, 2)\n",
      "domain/blocks/blocks/blocks3/task39.pddl (122, 39) (122, 2)\n",
      "domain/blocks/blocks/blocks3/task40.pddl (123, 39) (123, 2)\n",
      "domain/blocks/blocks/blocks3/task41.pddl (130, 39) (130, 2)\n",
      "domain/blocks/blocks/blocks3/task43.pddl (133, 39) (133, 2)\n",
      "domain/blocks/blocks/blocks3/task44.pddl (138, 39) (138, 2)\n",
      "domain/blocks/blocks/blocks3/task47.pddl (141, 39) (141, 2)\n",
      "domain/blocks/blocks/blocks3/task48.pddl (146, 39) (146, 2)\n",
      "domain/blocks/blocks/blocks3/task49.pddl (151, 39) (151, 2)\n"
     ]
    }
   ],
   "source": [
    "generate_problem_matrix(\"domain/blocks/domain.pddl\",\"domain/blocks/blocks/blocks3\",\"domain/blocks/plans\",\"tests/test_vectors\", \"blocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.  6. 12. ...  4.  4.  0.]\n",
      " [ 6.  5. 10. ...  4.  3.  0.]\n",
      " [ 1.  0.  0. ...  2.  0.  0.]\n",
      " ...\n",
      " [ 2.  2.  3. ...  3.  2.  0.]\n",
      " [ 0.  1.  2. ...  2.  0.  0.]\n",
      " [ 3.  0.  0. ...  2.  0.  0.]] [[ 4.  0.]\n",
      " [ 3.  0.]\n",
      " [ 2.  0.]\n",
      " [ 6.  1.]\n",
      " [ 5.  1.]\n",
      " [ 4.  1.]\n",
      " [ 3.  1.]\n",
      " [ 2.  1.]\n",
      " [ 8.  2.]\n",
      " [ 7.  2.]\n",
      " [ 6.  2.]\n",
      " [ 5.  2.]\n",
      " [ 4.  2.]\n",
      " [ 3.  2.]\n",
      " [ 2.  2.]\n",
      " [ 6.  3.]\n",
      " [ 5.  3.]\n",
      " [ 4.  3.]\n",
      " [ 3.  3.]\n",
      " [ 2.  3.]\n",
      " [ 6.  4.]\n",
      " [ 5.  4.]\n",
      " [ 4.  4.]\n",
      " [ 3.  4.]\n",
      " [ 2.  4.]\n",
      " [ 6.  5.]\n",
      " [ 5.  5.]\n",
      " [ 4.  5.]\n",
      " [ 3.  5.]\n",
      " [ 2.  5.]\n",
      " [ 2.  6.]\n",
      " [ 4.  7.]\n",
      " [ 3.  7.]\n",
      " [ 2.  7.]\n",
      " [ 8.  8.]\n",
      " [ 7.  8.]\n",
      " [ 6.  8.]\n",
      " [ 5.  8.]\n",
      " [ 4.  8.]\n",
      " [ 3.  8.]\n",
      " [ 2.  8.]\n",
      " [ 6.  9.]\n",
      " [ 5.  9.]\n",
      " [ 4.  9.]\n",
      " [ 3.  9.]\n",
      " [ 2.  9.]\n",
      " [ 4. 10.]\n",
      " [ 3. 10.]\n",
      " [ 2. 10.]\n",
      " [ 4. 11.]\n",
      " [ 3. 11.]\n",
      " [ 2. 11.]\n",
      " [ 8. 12.]\n",
      " [ 7. 12.]\n",
      " [ 6. 12.]\n",
      " [ 5. 12.]\n",
      " [ 4. 12.]\n",
      " [ 3. 12.]\n",
      " [ 2. 12.]\n",
      " [ 6. 13.]\n",
      " [ 5. 13.]\n",
      " [ 4. 13.]\n",
      " [ 3. 13.]\n",
      " [ 2. 13.]\n",
      " [ 8. 14.]\n",
      " [ 7. 14.]\n",
      " [ 6. 14.]\n",
      " [ 5. 14.]\n",
      " [ 4. 14.]\n",
      " [ 3. 14.]\n",
      " [ 2. 14.]\n",
      " [ 8. 15.]\n",
      " [ 7. 15.]\n",
      " [ 6. 15.]\n",
      " [ 5. 15.]\n",
      " [ 4. 15.]\n",
      " [ 3. 15.]\n",
      " [ 2. 15.]\n",
      " [ 4. 16.]\n",
      " [ 3. 16.]\n",
      " [ 2. 16.]\n",
      " [ 4. 17.]\n",
      " [ 3. 17.]\n",
      " [ 2. 17.]\n",
      " [ 6. 18.]\n",
      " [ 5. 18.]\n",
      " [ 4. 18.]\n",
      " [ 3. 18.]\n",
      " [ 2. 18.]\n",
      " [ 4. 19.]\n",
      " [ 3. 19.]\n",
      " [ 2. 19.]\n",
      " [ 4. 20.]\n",
      " [ 3. 20.]\n",
      " [ 2. 20.]\n",
      " [ 2. 21.]\n",
      " [ 4. 22.]\n",
      " [ 3. 22.]\n",
      " [ 2. 22.]\n",
      " [ 2. 23.]\n",
      " [ 6. 24.]\n",
      " [ 5. 24.]\n",
      " [ 4. 24.]\n",
      " [ 3. 24.]\n",
      " [ 2. 24.]\n",
      " [ 2. 25.]\n",
      " [ 4. 26.]\n",
      " [ 3. 26.]\n",
      " [ 2. 26.]\n",
      " [ 6. 27.]\n",
      " [ 5. 27.]\n",
      " [ 4. 27.]\n",
      " [ 3. 27.]\n",
      " [ 2. 27.]\n",
      " [ 4. 28.]\n",
      " [ 3. 28.]\n",
      " [ 2. 28.]\n",
      " [ 6. 29.]\n",
      " [ 5. 29.]\n",
      " [ 4. 29.]\n",
      " [ 3. 29.]\n",
      " [ 2. 29.]\n",
      " [ 2. 30.]\n",
      " [ 8. 31.]\n",
      " [ 7. 31.]\n",
      " [ 6. 31.]\n",
      " [ 5. 31.]\n",
      " [ 4. 31.]\n",
      " [ 3. 31.]\n",
      " [ 2. 31.]\n",
      " [ 4. 33.]\n",
      " [ 3. 33.]\n",
      " [ 2. 33.]\n",
      " [ 6. 34.]\n",
      " [ 5. 34.]\n",
      " [ 4. 34.]\n",
      " [ 3. 34.]\n",
      " [ 2. 34.]\n",
      " [ 4. 37.]\n",
      " [ 3. 37.]\n",
      " [ 2. 37.]\n",
      " [ 6. 38.]\n",
      " [ 5. 38.]\n",
      " [ 4. 38.]\n",
      " [ 3. 38.]\n",
      " [ 2. 38.]\n",
      " [ 6. 39.]\n",
      " [ 5. 39.]\n",
      " [ 4. 39.]\n",
      " [ 3. 39.]\n",
      " [ 2. 39.]]\n",
      "(151, 39) (151, 2)\n"
     ]
    }
   ],
   "source": [
    "results = np.load(\"tests/test_vectors/blocks.npz\")\n",
    "X = results['feature']\n",
    "Y = results['label']\n",
    "print(X,Y)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "from sklearn import svm, linear_model\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy import stats\n",
    "from svm import synthetic_data as sdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data has shape: (28, 55)\n",
      "cost has shape: (28, 2)\n",
      "\n",
      "first vector in data set:\n",
      " [23.  8. 17. 12. 20.  2.  3.  2.  1.  1.  0.  2.  1.  2.  1.  3.  3.  4.\n",
      "  1.  2.  4.  1.  1.  1.  3.  3.  1.  2.  4.  3.  2.  2.  3.  3.  4.  2.\n",
      "  4.  3.  1.  4.  2.  4.  2.  1.  4.  4.  1.  2.  2.  2.  2.  0.  1.  3.\n",
      "  4.]\n",
      "\n",
      "first 7 element in cost vector:\n",
      " [[ 8  0]\n",
      " [15  0]\n",
      " [ 8  1]\n",
      " [39  1]\n",
      " [58  1]\n",
      " [37  1]\n",
      " [34  1]]\n",
      "the index for problems:  [ 0  2 11 18 20 28]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "# p_index, test_data, cost = sdata.create_synthetic_data(25, 7, 100, 5, 64, 23, 3)\n",
    "p_index, test_data, cost = sdata.create_synthetic_data(5, 5, 100, 2, 10, 23, 3)\n",
    "print(f\"test_data has shape: {test_data.shape}\\ncost has shape: {cost.shape}\\n\")\n",
    "print(\"first vector in data set:\\n\",np.round(test_data[0]))\n",
    "cost_display = cost[0:min(len(cost),7)]\n",
    "print(\"\\nfirst 7 element in cost vector:\\n\",cost_display)\n",
    "print(\"the index for problems: \",p_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "implementation inspired from\n",
    "https://gist.github.com/agramfort/2071994\n",
    "\"\"\"\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import svm, linear_model\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "def transform_pairwise(X, y):\n",
    "    \"\"\"\n",
    "    Transforms data into pairs for convex relaxation of kendal rank correlation coef\n",
    "    In this method, all pairs are choosen, except for those that have the same target value or equal cost\n",
    "    Inputs\n",
    "    ----------\n",
    "    X : array, shape (n_samples, n_features)\n",
    "        The input feature vec of states from of several problems\n",
    "    y : array, shape (n_samples,) or (n_samples, 2)\n",
    "        The input cost vector. If it's a 2D array, the second column represents\n",
    "        the problem index\n",
    "    Returns\n",
    "    -------\n",
    "    X_trans : array, shape (k, n_feaures)\n",
    "        Difference between features of states (si - sj), only consider the state pair from the same problem\n",
    "    y_trans : array, shape (k,)\n",
    "        Output rank labels of values {-1, +1}, 1 represent si has potentially larger cost than sj (further away from goal)\n",
    "    \"\"\"\n",
    "    X_new = []\n",
    "    y_new = []\n",
    "    if y.ndim == 1:\n",
    "        y = np.c_[y, np.ones(y.shape[0])]\n",
    "    comb = itertools.combinations(range(X.shape[0]), 2)\n",
    "    for k, (i, j) in enumerate(comb):\n",
    "        if y[i, 0] == y[j, 0] or y[i, 1] != y[j, 1]:\n",
    "            # skip if they have the same cost or are from different problem group\n",
    "            continue\n",
    "        # otherwise, make the new pair-wise data\n",
    "        X_new.append(X[i] - X[j])\n",
    "        y_new.append(np.sign(y[i, 0] - y[j, 0])) # y = 1 if xi further away (larger cost), Vice Vesa\n",
    "        # randomly output some negative values for training purpose\n",
    "        if y_new[-1] != (-1) ** k:\n",
    "            y_new[-1] = - y_new[-1]\n",
    "            X_new[-1] = - X_new[-1]\n",
    "    return np.asarray(X_new), np.asarray(y_new)\n",
    "\n",
    "\n",
    "class RankSVM(svm.LinearSVC):\n",
    "    \"\"\"\n",
    "    Performs pairwise ranking svm with an underlying LinearSVC model\n",
    "    initialise with a C of regularization term\n",
    "    default using hinge loss\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, C = 1.0):\n",
    "        super(RankSVM, self).__init__()\n",
    "        self.C = C\n",
    "        self.loss = 'hinge'\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit a pairwise ranking model by first transfer it into pairwise than fitting\n",
    "        Inputs\n",
    "        ----------\n",
    "        X : array, shape (n_samples, n_features)\n",
    "        y : array, shape (n_samples,) or (n_samples, 2)\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        X_trans, y_trans = transform_pairwise(X, y)\n",
    "        super(RankSVM, self).fit(X_trans, y_trans)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict an ordering on X. For a list of n samples, this method\n",
    "        returns a list from 0 to n-1 with the relative order of the rows of X.\n",
    "        Inputs\n",
    "        ----------\n",
    "        X : array, shape (n_samples, n_features)\n",
    "        Returns\n",
    "        -------\n",
    "        rtn: array, shape (n_samples,)\n",
    "            Returns a list of integers representing the relative order of\n",
    "            the rows in X.\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'coef_'):\n",
    "            return np.argsort(np.dot(X, self.coef_.T).flatten())\n",
    "        else:\n",
    "            raise ValueError(\"Must call fit() prior to predict()\")\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Returns the accuracy for the rank prediction, from 0-1\n",
    "        \"\"\"\n",
    "        X_trans, y_trans = transform_pairwise(X, y)\n",
    "        return np.mean(super(RankSVM, self).predict(X_trans) == y_trans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_svm = RankSVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(151, 39) (151, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = np.load(\"tests/test_vectors/blocks.npz\")\n",
    "X = results['feature']\n",
    "Y = results['label']\n",
    "# print(X,Y)\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "\n",
    "my_svm.fit(X, Y)\n",
    "\n",
    "tx, ty = generate_training_data('domain/blocks/domain.pddl', 'domain/blocks/blocks/blocks3/task52.pddl', 'domain/blocks/plans/blocks3-task52_1800.out', 100)\n",
    "my_svm.predict(tx)\n",
    "my_svm.score(tx, ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.array([1,0,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f8dfbf299046a8c4b2110a11b95a75110c24cb97ab3ce5dec25bf2ee93348e3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
